---
title: "Song Lyric Challenges"
output:   
  html_document:
    df_print: paged
authors: Dipankar Lahiri
updated: "12/11/2024"
---

# Intro

For these challenges, we will analyse song lyrics from popular artists, including BTS, Taylor Swift, Beyonce and others, using Natural Language Processing and Sentiment Analysis. For challenge A and B, we will use the following dataset:

[Song Lyrics dataset on Kaggle](https://www.kaggle.com/datasets/deepshah16/song-lyrics-dataset)

This is located in the directory `data_challengeA+B`. The dataset has some duplication of songs as remixes, which will have the same or largely similar lyrics. You can attempt to deduplicate this data; otherwise, simply proceed to analyse the data but bear in mind this limitation. Also bear in mind that the lyric field sometimes also contains information about which artist is singing a lyric at any point.

The above dataset is not suitable for sentiment analysis, because it is missing punctuation/line breaks needed to break it into smaller units. So for Challenge C, on sentiment analysis, we will look at the following dataset, which breaks the data into lines:

[Taylor Swift lyrics dataset](https://www.kaggle.com/datasets/PromptCloudHQ/taylor-swift-song-lyrics-from-all-the-albums)

This is located in the directory `data_challengeC`.

# Challenge A: Analyse an artist’s lyrics

**Knowledge required:** You will need to use a Natural Language Processing (NLP) library, such as udpipe. You will also need basic knowledge of data frames.

**Skills gained:** Practice in NLP skills, focussing on Parts of Speech (POS) analysis.

For this challenge, pick a single artist from the datasets available in the directory `data_challengeA+B`. Excluding stop words, perform a parts of speech analysis on lemmas to identify the following about this artists song lyrics:

* What are the 10 most frequently used verbs?
* What are the 10 most frequently used nouns?
* What are the 10 most frequently used adjectives
* What are the 10 most frequently used adverbs?

```{r}
# Loading libraries, adding dataset, manipulating dataset to make it more amenable to analysis (making Coldplay's 60 albums levels, arranging songs by release date)

library(udpipe) 
library(dplyr)
library(tidyverse)
library(tm)
library(ggplot2)
library(tidytext)
library(stringr)

my_df <- read_csv("data_challengeA+B/ColdPlay.csv")
my_df <- my_df %>% select(-1)
my_df <- my_df %>%
  mutate(Album = factor(Album, exclude = NULL),
         Date = as.Date(Date, format = "%Y-%m-%d")) %>%
  arrange(Date)

# Deduplicating songs (to avoid over-representation of words from multiple versions of the same song)

my_df <- my_df %>%
  mutate(Title_clean = tolower(Title) %>%
                     str_replace_all("\\(.*\\)|\\[.*\\]", "") %>% 
                     str_replace_all("[^a-z0-9 ]", "") %>% 
                     str_trim()) %>%
  distinct(Title_clean, .keep_all = TRUE) %>%
  select(-Title_clean)

## Dataframe my_df now has 245 songs as rows, down from 344 in the csv file, which means 99 songs in the dataset were additional versions of same songs. It has columns for Title, Album, Year and Date. These can be used for future analysis, visualisations of individual songs.

# Tokenising lyrics into words, removing stopwords

my_df <- my_df %>%
  mutate(Lyric = str_replace_all(Lyric, "[^\\w\\s'’-]", " ") %>%
                   stringi::stri_trans_general("NFC"))

my_df_tokens <- my_df %>%
  unnest_tokens(word, Lyric, token = "regex", pattern = "([^\\w'-]+)") %>%
  filter(!str_detect(word, "^\\d+$")) 

data("stop_words")
stop_words_clean <- stop_words %>% filter(!str_detect(word, "'"))

my_df_tokens <- my_df_tokens %>%
  anti_join(stop_words_clean, by = "word") 


## Created new dataframe my_df_tokens. Here, all words from the Lyric column of my_df are split up and given separate rows, preserving contractions, possessives, and hyphenated words. Then, eliminated all stop word and numbers. This first creates a dataset of 42,267 rows, before filtering, resulting in 15,813 rows for all meaningful words. 

# Loading parts-of-speech model, tagging words in my_df_tokens with this model

ud_model <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model("/Users/dipankarlahiri/Desktop/College/Sem 2/Data Analysis and Collection/R/Projects/Training/Song lyrics/SongLyricChallenges/english-ewt-ud-2.5-191206.udpipe")

my_df_pos <- udpipe_annotate(ud_model, x = my_df_tokens$word) %>%
  as.data.frame()

my_df_pos <- my_df_pos %>%
  mutate(token = case_when(
    tolower(lag(token)) %in% c("do", "does", "did", "would", "should", "could", "ca") & tolower(token) == "n't" ~ paste0(lag(token), "n't"),
    tolower(lag(token)) %in% c("i", "you", "he", "she", "it", "we", "they") & token %in% c("'m", "'ve", "'ll", "'d", "'re") ~ paste0(lag(token), token),  
    tolower(lag(token)) %in% c("that", "who", "where", "how", "what", "there") & token == "'s" ~ paste0(lag(token), "'s"),
    tolower(lag(token)) == "o" & token == "'er" ~ "o'er",
    tolower(lag(token)) == "rock" & token == "'n" & lead(token) == "'roll" ~ "rock’n’roll",
    tolower(lag(token)) == "let" & token == "'s" ~ "let's",
    tolower(lag(token)) == "ma" & token == "'am" ~ "ma'am",
    tolower(lag(token)) == "y" & token == "'all" ~ "y'all",
    tolower(lag(token)) == "c" & token == "'mon" ~ "c'mon",
    tolower(lag(token)) == "would" & tolower(token) == "nt" ~ "wouldn't",
    tolower(lag(token)) == "could" & token == "'ve" ~ "could've",
    tolower(lag(token)) == "should" & token == "'ve" ~ "should've",
    lag(token) == "ca" & token == "n't" ~ "can't",  # Fix for "ca"
    TRUE ~ token
  )) %>%
  filter(!token %in% c("n't", "'m", "'ve", "'ll", "'d", "'s", "'n", "'roll", "'am", "'you", "nt", "'all", "'mon", "'", "ca"))

my_df_pos <- my_df_pos %>%
  filter(!(lag(token) == token & row_number() > 1))

## Created new dataframe my_df_pos by tagging words from my_df_tokens using a parts-of-speech model. This initially increased the row count to 18,000 due to the model splitting contractions, possessives, and compound words into separate tokens. To address this, merged key contractions and compound words back into single tokens while preserving their grammatical structure. After this adjustment, my_df_pos now has 16,127 rows.

```

Add more code chunks as needed, and document as you go.

# COLDPLAY'S TOP VERBS

```{r}

pos_counts <- my_df_pos %>%
  filter(upos %in% c("VERB", "NOUN", "ADJ", "ADV")) %>%
  count(upos, lemma, sort = TRUE)

top_verbs <- pos_counts %>% filter(upos == "VERB") %>% slice_head(n = 15)
list(Verbs = top_verbs)

```

# COLDPLAY'S TOP NOUNS

```{r}

top_nouns <- pos_counts %>% filter(upos == "NOUN") %>% slice_head(n = 15)
list (Nouns = top_nouns)
  

```

# COLDPLAY'S TOP ADJECTIVES

```{r}

top_adjectives <- pos_counts %>% filter(upos == "ADJ") %>% slice_head(n = 15)
list(Adjectives = top_adjectives)
  
```

# COLDPLAY'S TOP ADVERBS
```{r}
top_adverbs <- pos_counts %>% filter(upos == "ADV") %>% slice_head(n = 15)
list(Adverbs = top_adverbs)

```

What does this suggest to you about the themes that are covered in this artists lyrics? And what are the limitations of this approach? Write some reflections.



# Challenge B: Compare three artists

**Knowledge required:** You will need to use a Natural Language Processing (NLP) library, such as Spacy in Python, or udpipe in R. You will also need basic knowledge of data frames.

**Skills practised:** Practice in NLP skills, focussing on Parts of Speech (POS) analysis, interpreting results.

Perform a comparative analysis of at least two further artists, to identify the following:

How often are your first artists top 10 verb, noun, adjective and adverb lemmas used by these two artists?

What are the two additional artists top 10 verb, noun, adjective and adverb lemmas?

You might want to write a function for some of the steps to make some of this easier.

```{r}

```

```{r}

```

```{r}

```

How similar or different are the two new artist from the first artist? What does this tell you about the original artist you analysed? What would you attribute this to? Does it challenge any of your original ideas?

Can you change these measures, so it shows how frequently this word as opposed to other content words (e.g. as a percentage). Does this change your analysis?

```{r}

```

```{r}

```


# Challenge C: Sentiment analysis

**Knowledge required:** You will need to use a sentiment analysis library, such as nltk/vader in Python, or syuzhet in R. You will also need basic knowledge of data frames.

**Skills practised:** Use of and interpretation of sentiment analysis

For this article we need to use the dataset in the directory `data_challengeC`

Find a biographical article about Taylor Swift, and see how her career is split into different phases. Split the data into these phases (e.g. by year of release), and extract the lyrics from these to compare how sentiment has changed.

First calculate positive or negative sentiment for lyrics in these phases (line by line) and then create averages for these phases. Is there a meaningful change in sentiment between the chosen phases? How would you account for that change?


```{r}
library(syuzhet) #syuzhet is a useful library for sentiment analysis
```

```{r}

```

```{r}

```

Then see if you can perform more fine grained analysis of the sentiment of Taylor Swift's lyrics. For instance, how does lyric sentiment change, on average, across songs on an album? Do the results seem meaningful to you?

```{r}

```

```{r}

```

What are the limitations of this approach?

