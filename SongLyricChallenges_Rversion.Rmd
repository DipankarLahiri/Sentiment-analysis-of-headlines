---
title: "Comparing lyrics of Coldplay, Eminem, Drake"
output:   
  html_document:
    df_print: paged
authors: Dipankar Lahiri
updated: "12/11/2024"
---

# A word frequency analysis of the complete lyrics of Coldplay contrasted with Eminem and Drake throw up these broad findings:
# 1. 'Love' (218) is the most common word for Coldplay. 'World' (127), 'time' (112), 'day' (107), 'feel' (101) follow. Emotional reflection, temporality of experience and cosmic elements are the three broad themes that can be identified from Coldplay's most common words. 
# 2. Drake and Eminem's lyrics both show a tendency for more explicit language. Drake's favourite words are 'nigga', 'shit', 'girl', 'time', 'love', 'fuck', 'money', 'baby', 'life' and 'bitch'. Eminem's favourite words are 'shit', 'fuck', 'bitch', 'time', 'shady', 'ass', 'day', leave', 'feel' and 'girl'.
# 3. 'Real' is the most common adjective used by Drake. 'Bad' and 'white' are Eminem's most common adjectives. Coldplay's common adjectives, on the other hand, are 'wrong', 'true', 'cold', 'dark', 'beautiful', 'free'.
# 4. 'Feel', 'hear' and 'start' are three high-occuring verbs common between all three artists. Some unique common verbs for Coldplay are 'wait', 'stand', 'cry' and 'fly'. Use of the words 'leave', 'stay' and 'hit' is common for Drake and Eminem and not for Coldplay. 'Play' is a verb commonly used by Drake only, 'fuck' and 'kill' are verbs commonly used by Eminem only.
# 5. 'Damn' is the most common adverb used by Drake and Eminem. Coldplay's most common adverb is 'forever'.

-------------------------------------------------------------------------------------------------------------

```{r setup}

knitr::opts_chunk$set(dev = 'png')

library(udpipe) 
library(dplyr)
library(tidyverse)
library(tm)
library(ggplot2)
library(tidytext)
library(stringr)
library(forcats)
library(wordcloud)
library(RColorBrewer)
library(grid)
library(gridExtra)

my_df <- read_csv("data_challengeA+B/ColdPlay.csv")
my_df <- my_df %>% select(-1)
```

# Loading libraries, cleaning dataset of Coldplay's lyrics to make it amenable to analysis (making Coldplay's 60 albums levels, arranging songs by release date). Deduplicating songs (to avoid over-representation of words from multiple versions of the same song). Tokenising lyrics into words, removing stopwords.

```{r}
my_df <- my_df %>%
  mutate(Album = factor(Album, exclude = NULL),
         Date = as.Date(Date, format = "%Y-%m-%d")) %>%
  arrange(Date)

my_df <- my_df %>%
  mutate(Title_clean = tolower(Title) %>%
                     str_replace_all("\\(.*\\)|\\[.*\\]", "") %>% 
                     str_replace_all("[^a-z0-9 ]", "") %>% 
                     str_trim()) %>%
  distinct(Title_clean, .keep_all = TRUE) %>%
  select(-Title_clean)
```

# Dataframe my_df now has 245 songs as rows, down from 344 in the csv file, which means 99 songs in the dataset were additional versions of same songs. It has columns for Title, Album, Year and Date. These can be used for future analysis, visualisations.

```{r}
my_df <- my_df %>%
  mutate(Lyric = str_replace_all(Lyric, "[^\\w\\s'’-]", " ") %>%
                   stringi::stri_trans_general("NFC"))

my_df_tokens <- my_df %>%
  unnest_tokens(word, Lyric, token = "regex", pattern = "([^\\w'-]+)") %>%
  filter(!str_detect(word, "^\\d+$")) 

data("stop_words")
stop_words_clean <- stop_words %>% filter(!str_detect(word, "'"))

my_df_tokens <- my_df_tokens %>%
  anti_join(stop_words_clean, by = "word") 
```

# Created new dataframe my_df_tokens. Here, all words from the Lyric column of my_df are split up and given separate rows, preserving contractions, possessives, and hyphenated words. Then, eliminated all stop word and numbers. This first creates a dataset of 42,267 rows, before filtering, resulting in 15,813 rows for all meaningful words. 

# Loading parts-of-speech model, tagging words in my_df_tokens with this model

```{r}
ud_model <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model("/Users/dipankarlahiri/Desktop/College/Sem 2/Data Analysis and Collection/R/Projects/Training/Song lyrics/SongLyricChallenges/english-ewt-ud-2.5-191206.udpipe")

my_df_pos <- udpipe_annotate(ud_model, x = my_df_tokens$word) %>%
  as.data.frame()

my_df_pos <- my_df_pos %>%
  mutate(token = case_when(
    tolower(lag(token)) %in% c("do", "does", "did", "would", "should", "could", "ca") & tolower(token) == "n't" ~ paste0(lag(token), "n't"),
    tolower(lag(token)) %in% c("i", "you", "he", "she", "it", "we", "they") & token %in% c("'m", "'ve", "'ll", "'d", "'re") ~ paste0(lag(token), token),  
    tolower(lag(token)) %in% c("that", "who", "where", "how", "what", "there") & token == "'s" ~ paste0(lag(token), "'s"),
    tolower(lag(token)) == "o" & token == "'er" ~ "o'er",
    tolower(lag(token)) == "rock" & token == "'n" & lead(token) == "'roll" ~ "rock’n’roll",
    tolower(lag(token)) == "let" & token == "'s" ~ "let's",
    tolower(lag(token)) == "ma" & token == "'am" ~ "ma'am",
    tolower(lag(token)) == "y" & token == "'all" ~ "y'all",
    tolower(lag(token)) == "c" & token == "'mon" ~ "c'mon",
    tolower(lag(token)) == "would" & tolower(token) == "nt" ~ "wouldn't",
    tolower(lag(token)) == "could" & token == "'ve" ~ "could've",
    tolower(lag(token)) == "should" & token == "'ve" ~ "should've",
    lag(token) == "ca" & token == "n't" ~ "can't",  # Fix for "ca"
    TRUE ~ token
  )) %>%
  filter(!token %in% c("n't", "'m", "'ve", "'ll", "'d", "'s", "'n", "'roll", "'am", "'you", "nt", "'all", "'mon", "'", "ca"))

my_df_pos <- my_df_pos %>%
  filter(!(lag(token) == token & row_number() > 1))
```

# Created new dataframe my_df_pos by tagging words from my_df_tokens using a parts-of-speech model. This initially increased the row count to 18,000 due to the model splitting contractions, possessives, and compound words into separate tokens. To address this, merged key contractions and compound words back into single tokens while preserving their grammatical structure. After this adjustment, my_df_pos now has 16,127 rows.

# COLDPLAY'S TOP VERBS - feel, hear, wait, be, live, fly, talk, start, cry, stand

```{r}
pos_counts <- my_df_pos %>%
  filter(upos %in% c("VERB", "NOUN", "ADJ", "ADV")) %>%
  count(upos, lemma, sort = TRUE)

top_verbs <- pos_counts %>% filter(upos == "VERB") %>% slice_head(n = 15)
list(Verbs = top_verbs)
```

# COLDPLAY'S TOP NOUNS - love, world, time, day, life, light, sky, head, heart, sun

```{r}
top_nouns <- pos_counts %>% filter(upos == "NOUN") %>% slice_head(n = 15)
list (Nouns = top_nouns)
```

# COLDPLAY'S TOP ADJECTIVES - close, wrong, true, alright, careful, dark, cold, strong, beautiful, free

```{r}
top_adjectives <- pos_counts %>% filter(upos == "ADJ") %>% slice_head(n = 15)
list(Adjectives = top_adjectives)
```

# COLDPLAY'S TOP ADVERBS - forever, inside, here, all, slowly, underneath, pretty, where

```{r}
top_adverbs <- pos_counts %>% filter(upos == "ADV") %>% slice_head(n = 15)
list(Adverbs = top_adverbs)
```

# COLDPLAY'S TOP WORDS OVERALL - love (218), world (127), time (112), day (107), feel (101)

# REFLECTIONS (Top 3 themes): 
# 'Love' is the most common word. 'Feel' is the only non-noun word to occur more than 100 times. 'Life' and 'Heart' are other top words that can be said to fall under a broad theme of emotional reflection.
# 'Time' and 'Day', two of the other top 5 words overall, can be seen to represent a theme of temporality. 'Forever', the most used adverb, 'Wait' and 'Start' fall under this theme.
# 'World', the second most common word overall, represents a theme of the cosmos. This includes natural elements like 'Light', 'Sky' and 'Sun'.

# Limitations: 1> Counting word frequencies can often miss the context in which the word is used. 2> Words being misclassified (Eg: lose as adjective) suggest limitations of the POS tagging model and the appearance of words like 'Se' suggest limitations in the lemmatisation.

-------------------------------------------------------------------------------------------------------------

# COMPARING COLDPLAY'S LYRICS WITH DRAKE & EMINEM

# DRAKE
```{r}

my_df_drake <- read_csv("data_challengeA+B/Drake.csv")
my_df_drake <- my_df_drake %>% select(-1)
my_df_drake <- my_df_drake %>%
  mutate(Album = factor(Album, exclude = NULL),
         Date = as.Date(Date, format = "%Y-%m-%d")) %>%
  arrange(Date)

my_df_drake <- my_df_drake %>%
  mutate(Title_clean = tolower(Title) %>%
                     str_replace_all("\\(.*\\)|\\[.*\\]", "") %>% str_replace_all("[^a-z0-9 ]", "") %>% 
                     str_trim()) %>%
  distinct(Title_clean, .keep_all = TRUE) %>%
  select(-Title_clean)

my_df_drake <- my_df_drake %>%
  mutate(Lyric = str_replace_all(Lyric, "[^\\w\\s'’-]", " ") %>%
                   stringi::stri_trans_general("NFC"))

my_df_drake_tokens <- my_df_drake %>%
  unnest_tokens(word, Lyric, token = "regex", pattern = "([^\\w'-]+)") %>%
  filter(!str_detect(word, "^\\d+$")) 

my_df_drake_tokens <- my_df_drake_tokens %>%
  anti_join(stop_words_clean, by = "word") 

my_df_drake_pos <- udpipe_annotate(ud_model, x = my_df_drake_tokens$word) %>%
  as.data.frame()

my_df_drake_pos <- my_df_drake_pos %>%
  mutate(token = case_when(
    tolower(lag(token)) %in% c("do", "does", "did", "would", "should", "could", "ca") & tolower(token) == "n't" ~ paste0(lag(token), "n't"),
    tolower(lag(token)) %in% c("i", "you", "he", "she", "it", "we", "they") & token %in% c("'m", "'ve", "'ll", "'d", "'re") ~ paste0(lag(token), token),  
    tolower(lag(token)) %in% c("that", "who", "where", "how", "what", "there") & token == "'s" ~ paste0(lag(token), "'s"),
    tolower(lag(token)) == "o" & token == "'er" ~ "o'er",
    tolower(lag(token)) == "rock" & token == "'n" & lead(token) == "'roll" ~ "rock’n’roll",
    tolower(lag(token)) == "let" & token == "'s" ~ "let's",
    tolower(lag(token)) == "ma" & token == "'am" ~ "ma'am",
    tolower(lag(token)) == "y" & token == "'all" ~ "y'all",
    tolower(lag(token)) == "c" & token == "'mon" ~ "c'mon",
    tolower(lag(token)) == "would" & tolower(token) == "nt" ~ "wouldn't",
    tolower(lag(token)) == "could" & token == "'ve" ~ "could've",
    tolower(lag(token)) == "should" & token == "'ve" ~ "should've",
    lag(token) == "ca" & token == "n't" ~ "can't",  
    TRUE ~ token
  )) %>%
  filter(!token %in% c("n't", "'m", "'ve", "'ll", "'d", "'s", "'n", "'roll", "'am", "'you", "nt", "'all", "'mon", "'", "ca"))

my_df_drake_pos <- my_df_drake_pos %>%
  filter(!(lag(token) == token & row_number() > 1))

pos_counts_drake <- my_df_drake_pos %>%
  filter(upos %in% c("VERB", "NOUN", "ADJ", "ADV")) %>%
  count(upos, lemma, sort = TRUE)

top_verbs_drake <- pos_counts_drake %>% filter(upos == "VERB") %>% slice_head(n = 15)
list(Verbs = top_verbs_drake)

top_nouns_drake <- pos_counts_drake %>% filter(upos == "NOUN") %>% slice_head(n = 15)
list (Nouns = top_nouns_drake)

top_adjectives_drake <- pos_counts_drake %>% filter(upos == "ADJ") %>% slice_head(n = 15)
list (Adjectives = top_adjectives_drake)

top_adverbs_drake <- pos_counts_drake %>% filter(upos == "ADV") %>% slice_head(n = 15)
list(Adverbs = top_adverbs_drake)

```

# DRAKE TOP WORDS OVERALL - Nigga, Shit, Girl, Time, Love, Fuck, Money, Baby, Life, Bitch (all nouns)
# DRAKE TOP NON-NOUNS - Real (ADJ - 268), Feel (VERB - 259), Start (VERB - 240), Leave (VERB - 233), Hear (Verb - 200)

# EMINEM

```{r}

my_df_eminem <- read_csv("data_challengeA+B/Eminem.csv")
my_df_eminem <- my_df_eminem %>% select(-1)
my_df_eminem <- my_df_eminem %>%
  mutate(Album = factor(Album, exclude = NULL),
         Date = as.Date(Date, format = "%Y-%m-%d")) %>%
  arrange(Date)

my_df_eminem <- my_df_eminem %>%
  mutate(Title_clean = tolower(Title) %>%
                     str_replace_all("\\(.*\\)|\\[.*\\]", "") %>% str_replace_all("[^a-z0-9 ]", "") %>% 
                     str_trim()) %>%
  distinct(Title_clean, .keep_all = TRUE) %>%
  select(-Title_clean)

my_df_eminem <- my_df_eminem %>%
  mutate(Lyric = str_replace_all(Lyric, "[^\\w\\s'’-]", " ") %>%
                   stringi::stri_trans_general("NFC"))

my_df_eminem_tokens <- my_df_eminem %>%
  unnest_tokens(word, Lyric, token = "regex", pattern = "([^\\w'-]+)") %>%
  filter(!str_detect(word, "^\\d+$")) 

my_df_eminem_tokens <- my_df_eminem_tokens %>%
  anti_join(stop_words_clean, by = "word") 

my_df_eminem_pos <- udpipe_annotate(ud_model, x = my_df_eminem_tokens$word) %>%
  as.data.frame()

my_df_eminem_pos <- my_df_eminem_pos %>%
  mutate(token = case_when(
    tolower(lag(token)) %in% c("do", "does", "did", "would", "should", "could", "ca") & tolower(token) == "n't" ~ paste0(lag(token), "n't"),
    tolower(lag(token)) %in% c("i", "you", "he", "she", "it", "we", "they") & token %in% c("'m", "'ve", "'ll", "'d", "'re") ~ paste0(lag(token), token),  
    tolower(lag(token)) %in% c("that", "who", "where", "how", "what", "there") & token == "'s" ~ paste0(lag(token), "'s"),
    tolower(lag(token)) == "o" & token == "'er" ~ "o'er",
    tolower(lag(token)) == "rock" & token == "'n" & lead(token) == "'roll" ~ "rock’n’roll",
    tolower(lag(token)) == "let" & token == "'s" ~ "let's",
    tolower(lag(token)) == "ma" & token == "'am" ~ "ma'am",
    tolower(lag(token)) == "y" & token == "'all" ~ "y'all",
    tolower(lag(token)) == "c" & token == "'mon" ~ "c'mon",
    tolower(lag(token)) == "would" & tolower(token) == "nt" ~ "wouldn't",
    tolower(lag(token)) == "could" & token == "'ve" ~ "could've",
    tolower(lag(token)) == "should" & token == "'ve" ~ "should've",
    lag(token) == "ca" & token == "n't" ~ "can't",  
    TRUE ~ token
  )) %>%
  filter(!token %in% c("n't", "'m", "'ve", "'ll", "'d", "'s", "'n", "'roll", "'am", "'you", "nt", "'all", "'mon", "'", "ca"))

my_df_eminem_pos <- my_df_eminem_pos %>%
  filter(!(lag(token) == token & row_number() > 1))

pos_counts_eminem <- my_df_eminem_pos %>%
  filter(upos %in% c("VERB", "NOUN", "ADJ", "ADV")) %>%
  count(upos, lemma, sort = TRUE)

top_verbs_eminem <- pos_counts_eminem %>% filter(upos == "VERB") %>% slice_head(n = 15)
list(Verbs = top_verbs_eminem)

top_nouns_eminem <- pos_counts_eminem %>% filter(upos == "NOUN") %>% slice_head(n = 15)
list (Nouns = top_nouns_eminem)

top_adjectives_eminem <- pos_counts_eminem %>% filter(upos == "ADJ") %>% slice_head(n = 15)
list (Adjectives = top_adjectives_eminem)

top_adverbs_eminem <- pos_counts_eminem %>% filter(upos == "ADV") %>% slice_head(n = 15)
list(Adverbs = top_adverbs_eminem)

```

# EMINEM TOP WORDS OVERALL - Shit, Fuck, Bitch, Time, Shady, Ass, Day, Leave (Verb), Feel (Verb), Girl

# Coldplay’s most common words are about emotions and abstract themes like love, world, time, life. Their verbs (feel, hear, wait, live, cry) suggest introspection. Drake and Eminem, however, lean heavily on slang and explicit language. Their top words include nigga, shit, bitch, fuck, money, and baby — terms tied to rap’s cultural and linguistic identity. Among adjectives, the word 'real' stands out in Drake's lyrics. Verbs are more common across all artists - one noticeable exception is the use of the words 'leave' and 'stay' by Drake and Eminem and not Coldplay.


```{r}

set.seed(42)

my_df_drake_pos_sample <- my_df_drake_pos %>% sample_n(16127)
my_df_eminem_pos_sample <- my_df_eminem_pos %>% sample_n(16127)

pos_counts_coldplay <- my_df_pos %>%
  filter(upos %in% c("VERB", "NOUN", "ADJ", "ADV")) %>%
  count(upos, lemma, sort = TRUE) %>%
  mutate(Artist = "Coldplay")

pos_counts_drake <- my_df_drake_pos_sample %>%
  filter(upos %in% c("VERB", "NOUN", "ADJ", "ADV")) %>%
  count(upos, lemma, sort = TRUE) %>%
  mutate(Artist = "Drake")

pos_counts_eminem <- my_df_eminem_pos_sample %>%
  filter(upos %in% c("VERB", "NOUN", "ADJ", "ADV")) %>%
  count(upos, lemma, sort = TRUE) %>%
  mutate(Artist = "Eminem")

top_words_all <- bind_rows(pos_counts_coldplay, pos_counts_drake, pos_counts_eminem)
top_words_all <- top_words_all %>%
  filter(upos %in% c("NOUN", "VERB", "ADJ", "ADV")) %>%
  group_by(Artist, upos) %>%
  slice_max(n = 10, order_by = n) %>% 
  ungroup()

ggplot(top_words_all, aes(x = fct_reorder(lemma, n), y = n, fill = Artist)) +
  geom_col(position = "dodge") +
  facet_wrap(~ upos, scales = "free") +
  coord_flip() +
  labs(title = "Top 10 Words in Lyrics by POS Category",
       x = "Words",
       y = "Frequency") +
  theme_minimal()

```

```{r}

plot_wordcloud <- function(artist_name, color_palette) {
  data_subset <- subset(top_words_all, Artist == artist_name)
  wordcloud(words = data_subset$lemma, freq = data_subset$n, 
            scale = c(2, 0.4),
            min.freq = 2,
            max.words = 120,
            random.order = FALSE, 
            colors = brewer.pal(8, color_palette))
}

plot_wordcloud("Coldplay", "Blues")
plot_wordcloud("Drake", "Reds")
plot_wordcloud("Eminem", "Purples")

```
