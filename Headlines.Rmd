---
title: "News Headline Analysis"
author: "Dipankar Lahiri"
date: "2025-03-26"
output: html_document
---

# This is an analysis of two datasets of news headlines to find their most common words, phrases, emotions and sentiments. The first dataset is a pre-existing collection of 23,481 fake political news headlines collected in 2017 from websites flagged as unreliable by Politifact, an US-based fact-checking organisation (Dataset link here -> https://www.kaggle.com/datasets/emineyetm/fake-news-detection-datasets). The second dataset is a collection of 1,200 headlines scraped in March 2025 from one particular website covering Indian sports news. The NRC Emotional Lexicon was used for emotion and sentiment analysis, a list which annotates 14,182 English words into two sentiments - positive, negative - and eight emotions - anger, fear, anticipation, trust, surprise, sadness, joy, and disgust. The differences in results for the two datasets offer some interesting insights into the nature of headlines for fake news and sports news.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(rvest)
library(dplyr)
library(readr)
library(stringr)
library(tidyr)
library(tm)
library(SnowballC)
library(textclean)
library(tidytext)
library(readxl)
library(ggplot2)
library(reshape2)
library(wordcloud)
library(wordcloud2)
library(RColorBrewer)
library(igraph)
library(ggraph)
library(networkD3)
library(treemap)
library(topicmodels)
library(wdman)
library(htmlwidgets)
library(plotly)

```

# MAIN FINDINGS

# 1. Fake news headlines are dominated by political figures and media references. "Video" and "Trump" are the most frequent words, with heavy use of clickbait phrases like "You won’t believe" and "The truth about." Political bigrams and trigrams such as "White House," "Black Lives Matter," and "Fox News" appear frequently. Sports news headlines are dominated by geographical entities and competition names. The most common words include "India," "Asian," and "World," with frequent mentions of tournaments ("Olympics 2024," "Asian Games") and top athletes. Unlike fake news, clickbait phrases are absent. 

# 2. Fake news headlines often recycle political narratives, with some five- and six-word phrases recurring multiple times (e.g., "Bill Clinton is a rapist"). Sports headlines, while containing a large number of unique pentagrams and hexagrams, have almost no repeated multi-word phrases.

# 3. Fake news headlines lean negative; sports headlines are overwhelmingly positive - Fake news shows a higher mean negative sentiment (0.21) than positive (0.17), with "fear" and "surprise" as dominant emotions. In contrast, sports headlines have a much stronger positive sentiment (0.22) than negative (0.11), with "trust" and "anticipation" being the most common emotions. "Joy" is the emotion least present in fake news headlines, "disgust" is the emotion least present in sports headlines.

# 4. In fake news, "fear" is the emotion most linked with negativity, while "trust" and "anticipation" drive positive sentiment. "Surprise" stands out as the most independently occurring emotion, meaning many headlines evoke surprise without simultaneously expressing any other emotion or sentiment. This suggests that fake news headlines frequently rely on shock value to capture attention. In sports news, positive sentiment is dominant, with "joy" and "trust" frequently appearing together. However, unlike fake news, where specific emotions strongly define sentiment, sports headlines often frame positivity in a more neutral way. Many positive sports headlines do not evoke any of the eight classified emotions, suggesting that positivity is often conveyed without strong emotional framing in sports reporting.

---------------------------------

# FAKE NEWS DATASET

# Cleaning headlines for analysis, finding frequencies of words and word combinations.

```{r}

headlines_df <- read_csv("Fake.csv")
headlines_df$title <- tolower(headlines_df$title)
tidy_headlines <- headlines_df %>%
  unnest_tokens(word, title)
tidy_headlines <- tidy_headlines %>%
  anti_join(stop_words)
tidy_headlines <- tidy_headlines %>%
  filter(nchar(word) > 1)
tidy_headlines$word <- gsub("[[:punct:]]", "", tidy_headlines$word)

word_freq <- tidy_headlines %>%
  count(word, sort = TRUE)
head(word_freq, 10)

bigrams <- headlines_df %>%
  unnest_tokens(bigram, title, token = "ngrams", n = 2) %>%
  count(bigram, sort = TRUE)
head(bigrams, 10)

trigrams <- headlines_df %>%
  unnest_tokens(trigram, title, token = "ngrams", n = 3) %>%
  count(trigram, sort = TRUE)
head(trigrams, 10)

quadgrams <- headlines_df %>%
  unnest_tokens(quadgram, title, token = "ngrams", n = 4) %>%
  count(quadgram, sort = TRUE)
head(quadgrams, 10)

pentagrams <- headlines_df %>%
  unnest_tokens(pentagram, title, token = "ngrams", n = 5) %>%
  count(pentagram, sort = TRUE)
head(pentagrams, 10)

hexagrams <- headlines_df %>%
  unnest_tokens(hexagram, title, token = "ngrams", n = 6) %>%
  count(hexagram, sort = TRUE)
head(hexagrams, 10)

```

# WORD FREQUENCY VISUALISATIONS

# SINGLE WORDS - Word Cloud, Bar chart

```{r}

set.seed(123)
wordcloud(words = word_freq$word, 
          freq = word_freq$n, 
          min.freq = 2, 
          max.words = 100, 
          random.order = FALSE, 
          rot.per = 0.3, 
          colors = brewer.pal(8, "Dark2"))

top_words <- word_freq %>% slice_max(n, n = 15)
ggplot(top_words, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Top Words in Fake News Headlines",
       x = "Words", y = "Frequency") +
  theme_minimal()

```

# 'Video' and 'Trump' are the two most common words in the fake news headlines dataset, both words occurring at least four times as much as other common words. 'Video' is mostly used at the beginning or end of headlines, denoting that the article contains a video (Eg -  While Honoring Native American Code Talkers, Trump Called Elizabeth Warren ‘Pocahontas’ (VIDEO)). 'Tweets' is another word used in the same way - to denote the content type and source of the news - but this occurs ten times less than 'video', suggesting visual elements are ten times more common than tweets for fake news. 'Watch' is the third most common word, this is used as a substitute for 'video' (Eg -  WATCH: Nicolle Wallace Takes Trump To The Woodshed For Backing Pedophile Moore). 

# The next few popular words are all related to US Presidential candidates - 'Obama', 'Hillary', 'Trumps', 'President', 'Clinton'. It is interesting that the only Presidential candidate to be mentioned more by first name than by surname is a woman, but this could be explained by the sameness of her surname with the President preceding Obama. 

# Other common words occurring less than 1000 times include 'breaking' (another word like 'video' preceding headlines used to denote the type of news), 'black' and 'white'. While 'black' and 'white' both occur roughly the same number of times, 'black' is the more distinct connotator of race (Eg - Florida Student In Deep Sh*t After Trying To Sell Black Classmates Into Slavery On Craigslist). 'White', on the other hand, is a less racial term, more than a third of its occurences are in relation to the White House.  

# TWO-WORD PHRASES - Word Cloud, Bar Chart

``` {r}

bigrams %>%
  filter(n > 5) %>% 
  wordcloud2(size = 0.5)

top_bigrams <- bigrams %>% slice_max(n, n = 15)
ggplot(top_bigrams, aes(x = reorder(bigram, n), y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Top Bigrams in Fake News Headlines",
       x = "Bigrams", y = "Frequency") +
  theme_minimal()

```

# Various versions of Donald Trump's name dominates two-word phrases in the fake news dataset. Some other common bigrams include governmental terms like 'White House', 'Supreme Court'; some second-tier US political leaders like Bernie Sanders, Ted Cruz, Kellyanne Conway, Elizabeth Warren, Nancy Peloisi; and some politically loaded terms like 'fake news', 'illegal alien' and 'Muslim refugees'. 'Fox News' is another common bigram, but the nature of this term's occurrences is best illustrated in the next section, when its mentions can be contrasted with a popular trigram, 'New York Times'.

# THREE-WORD PHRASES - Bar Chart, Sankey Network

```{r}

plot_ngrams <- function(df, title, top_n = 20) {
  df %>%
    as_tibble() %>%  
    arrange(desc(n)) %>%  
    head(top_n) %>%  
    ggplot(aes(x = reorder(!!sym(names(df)[1]), n), y = n)) +  
    geom_col(fill = "steelblue") +
    coord_flip() +
    labs(title = title, x = "N-Gram", y = "Frequency") +
    theme_minimal()
}

plot_ngrams(trigrams, "Top 20 Trigrams")

sankey_data_trigram <- trigrams %>%
  separate(trigram, into = c("word1", "word2", "word3"), sep = " ", remove = FALSE) %>%
  top_n(20, wt = n)
links_trigram <- data.frame(
  source = c(sankey_data_trigram$word1, sankey_data_trigram$word2),
  target = c(sankey_data_trigram$word2, sankey_data_trigram$word3),
  value = rep(sankey_data_trigram$n, 2)
)
nodes_trigram <- data.frame(name = unique(c(links_trigram$source, links_trigram$target)))
links_trigram$source <- match(links_trigram$source, nodes_trigram$name) - 1
links_trigram$target <- match(links_trigram$target, nodes_trigram$name) - 1
saveWidget(sankeyNetwork(Links = links_trigram, Nodes = nodes_trigram, Source = "source", Target = "target",
                        Value = "value", NodeID = "name", fontSize = 12, nodeWidth = 30), 
           "sankey_diagram.html", selfcontained = TRUE)

```

# The most common trigram is 'Black Lives Matter'. The BLM social movement was at its peak during the time of this dataset curation. Notably, this phrase is used as a character in headlines instead of as an SEO keyword (Eg -  Mark Zuckerberg Stands Strong With Black Lives Matter – Reprimands Racist Facebook Employees). The second most common trigram is an SEO keyword, 'boiler room ep', used by one particular website.

# 'You won't believe' and 'The truth about' turn out to be popular three-word clickbait phrases in fake news headlines. 'You won't believe' occurs 75 times in the dataset (Eg - Susan Rice Refuses To Testify Before The Senate…You Won’t Believe The Reason Why!). Some headlines with this phrase often contain multiple clickbait elements (Eg - YOU WON’T BELIEVE THIS: WATCH DONNA BRAZILE Defend Her Decision to Cheat by Leaking Questions to Hillary [Video]).

# 'CNN' (more than 500 mentions), 'Fox News' (more than 200 mentions) and 'New York Times' (more than 50 mentions) are some popular news companies to find frequent mention, with the headlines usually mocking the companies. Examples: 1> ‘Man Bites Dog’: New York Times Does Some Actual Journalism. 2> Fox News Frantically Tries To Cover Up Trump’s 13th Golf Trip And Absolutely NOBODY Is Buying It. CNN, on the other hand, finds favourable mention several times (Eg - CNN Just Went After Trump For His Latest Meltdown, And It Was Unbelievably BRILLIANT (VIDEO)).

# FOUR-WORD PHRASES

```{r}

plot_ngrams(quadgrams, "Top 20 Quadgrams")

sankey_data_quadgram <- quadgrams %>%
  separate(quadgram, into = c("word1", "word2", "word3", "word4"), sep = " ", remove = FALSE) %>%
  top_n(20, wt = n)

links_quadgram <- data.frame(
  source = c(sankey_data_quadgram$word1, sankey_data_quadgram$word2, sankey_data_quadgram$word3),
  target = c(sankey_data_quadgram$word2, sankey_data_quadgram$word3, sankey_data_quadgram$word4),
  value = rep(sankey_data_quadgram$n, 3)
)

nodes_quadgram <- data.frame(name = unique(c(links_quadgram$source, links_quadgram$target)))
links_quadgram$source <- match(links_quadgram$source, nodes_quadgram$name) - 1
links_quadgram$target <- match(links_quadgram$target, nodes_quadgram$name) - 1

sankeyNetwork(Links = links_quadgram, Nodes = nodes_quadgram, Source = "source", Target = "target",
              Value = "value", NodeID = "name", fontSize = 12, nodeWidth = 30)

```

# The most common phrases of four words or more in US fake news headlines all refer to someone handing someone's ass! As for non-expletive-laden terms, 'Make America great again' and 'Black Lives Matter terrorists' - two political terms used by Republican-leaning news websites - are the most common.

# A few other clickbait phrases are evident here: 'Watch what happens when' turns out to be the most common phrase starting with 'watch'. 'Want you to see' is a variation of this phrase (Eg - Here’s The Ted Cruz Ad Starring An Adult Film Star He Doesn’t Want You To See (VIDEO)). 'You need to know' and 'need to know about' are variations of the same clickbait phrase that occurs around 25 times (Eg - TOP TEN Clinton Scandals That Wikileaks Exposed And That YOU Need To Know About).

# FIVE-WORD PHRASES - Network Graph

```{r}

filtered_ngrams <- pentagrams %>%
  filter(n > 5) %>%
  separate(pentagram, into = c("w1", "w2", "w3", "w4", "w5"), sep = " ", remove = FALSE)

graph <- graph_from_data_frame(filtered_ngrams, directed = FALSE)

ggraph(graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
  geom_node_point(color = "blue", size = 3) +
  geom_node_text(aes(label = name), repel = TRUE, size = 3) +
  theme_minimal()

```

# The number of occurrences go down as we move into pentagrams and hexagrams, but the fact that some 5-word or 6-word phrases occur multiple times in headlines at all could merit further investigation. A few curious examples shown in the above network graph are 'NFL players disrespecting our flag' (6 times) and 'Bill Clinton is a rapist' (8 times). Examples show these phrases can be seen as having gained currency as recognisable entities (like the BLM social movement) for a short while but then died down.

# SIX-WORD PHRASES - Treemap

```{r}

hexagram_data <- hexagrams %>%
  top_n(20, wt = n) %>%
  separate(hexagram, into = c("w1", "w2", "w3", "w4", "w5", "w6"), sep = " ", remove = FALSE) %>%
  mutate(category = paste(w1, w2, sep = "_"))

treemap(hexagram_data,
        index = "hexagram",
        vSize = "n",
        vColor = "n",
        type = "index",
        palette = "Blues",
        title = "Hexagram Treemap")

```

# 'Everything you need to know' is a common hexagram, as shown in the treemap. Examples show this is a term used by websites on both side of the political spectrum - 1> THIS ONE PICTURE Tells You Everything You Need To Know About The Muslim Refugee Invasion. 2> This Single Anecdote Tells You EVERYTHING You Need To Know About Trump Voters.

## EMOTION DETECTION IN HEADLINES

```{r}

nrc_lexicon <- read_excel("~/Downloads/NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations.xlsx")
nrc_cleaned <- nrc_lexicon %>%
  select(Word = `English (en)...1`, 
         Positive, Negative, Anger, Anticipation, 
         Disgust, Fear, Joy, Sadness, Surprise, Trust)
nrc_cleaned <- nrc_cleaned %>%
  rename(word = Word)

headlines_emotions <- tidy_headlines %>%
  inner_join(nrc_cleaned, by = "word") %>%
  group_by(id = row_number()) %>%
  summarize(across(c(Positive:Trust), sum, na.rm = TRUE))
headlines_emotions_df <- headlines_df %>%
  mutate(id = row_number()) %>%
  left_join(headlines_emotions, by = "id")

headlines_emotions_df %>%
  summarize(across(Positive:Trust, mean, na.rm = TRUE))

```

# The mean negative sentiment of the fake news dataset is 0.21 and the mean positive sentiment is 0.17, suggesting the negative sentiment is contained more. Surprise (0.18) and fear (0.16) are the emotions most represented. Joy (0.07) is the least represented emotion.

# EMOTION VISUALISATION

```{r}

headlines_emotions_df %>%
  summarize(across(Positive:Trust, sum, na.rm = TRUE)) %>%
  pivot_longer(cols = everything(), names_to = "Emotion", values_to = "Count") %>%
  ggplot(aes(x = reorder(Emotion, -Count), y = Count, fill = Emotion)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = "Overall Emotional Distribution in Headlines",
       x = "Emotion",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

headlines_emotions_filtered <- headlines_emotions_df %>%
  filter(rowSums(select(., Positive:Trust) > 0) > 0)
heatmap_data <- as.matrix(headlines_emotions_filtered %>% select(Positive:Trust))
heatmap(heatmap_data, 
        scale = "column", 
        col = heat.colors(256), 
        margins = c(5, 10), 
        xlab = "Emotion", 
        ylab = "Headline",
        main = "Emotion Heatmap of Fake News Headlines")


```

# The heatmap shows how sentiments and emotions often co-occur in the headlines. Trust and anticipation are the most common emotions co-occurring with the positive sentiment (Eg - Mormon tabernacle choir singer quits because singing for Trump would be like singing for Hitler). Fear is the most common emotion co-occurring with the negative sentiment (Eg - Watch: Trump minion makes huge slip about who will pay for infrastructure spending). Surprise is the emotion occurring most independently, or in other words, it has positive scores for the most headlines which do not have scores for any other emotion or sentiment (Eg - Trump gets trashed for justifying Don Jr’s collusion with Russia: ‘That’s politics!’).

---------------------------------

# SCRAPING HEADLINES FROM WEBSITE 'THE BRIDGE'

```{r}

base_url <- "https://thebridge.in/latest"
all_headlines <- c()  

scrape_page <- function(url) {
  page_html <- read_html(url)
  h3_headlines <- page_html %>%
    html_nodes("h3.bd-heading-text") %>%
    html_text(trim = TRUE)
  h4_headlines <- page_html %>%
    html_nodes("h4.bd-heading-text") %>%
    html_text(trim = TRUE)
all_headlines <- c(h3_headlines, h4_headlines)
  
  return(all_headlines)
}

for (i in 1:1000) {
  page_url <- ifelse(i == 1, base_url, paste0(base_url, "/", i))
  cat("Scraping URL:", page_url, "\n")
  headlines <- scrape_page(page_url)
  all_headlines <- c(all_headlines, headlines)
  cat("Scraped page", page_url, "with", length(headlines), "headlines.\n")
}

bridge_headlines_df <- data.frame(headline = all_headlines)

```

# CREATING A FUNCTION FOR AUTOMATION OF ANALYSIS

```{r}

process_headlines_full <- function(df, text_col, lexicon) {
  library(tidyverse)
  library(tidytext)
  library(rvest)
  library(wordcloud)
  library(wordcloud2)
  library(RColorBrewer)
  library(igraph)
  library(ggraph)
  library(networkD3)
  library(treemapify)
  library(reshape2)
  
  df <- df %>% 
    rename(title = {{ text_col }}) %>%
    mutate(title = tolower(title))
  
  tidy_df <- df %>%
    unnest_tokens(word, title) %>%
    anti_join(stop_words, by = "word") %>%
    filter(nchar(word) > 1) %>%
    mutate(word = gsub("[[:punct:]]", "", word))
  
  word_freq <- tidy_df %>% 
    count(word, sort = TRUE)
  
  bigrams <- df %>% 
    unnest_tokens(bigram, title, token = "ngrams", n = 2) %>%
    count(bigram, sort = TRUE)
  
  trigrams <- df %>% 
    unnest_tokens(trigram, title, token = "ngrams", n = 3) %>%
    count(trigram, sort = TRUE)
  
  quadgrams <- df %>% 
    unnest_tokens(quadgram, title, token = "ngrams", n = 4) %>%
    count(quadgram, sort = TRUE)
  
  pentagrams <- df %>% 
    unnest_tokens(pentagram, title, token = "ngrams", n = 5) %>%
    count(pentagram, sort = TRUE)
  
  hexagrams <- df %>% 
    unnest_tokens(hexagram, title, token = "ngrams", n = 6) %>%
    count(hexagram, sort = TRUE)
  
    wordcloud(words = word_freq$word, 
            freq = word_freq$n, 
            min.freq = 2, 
            max.words = 100, 
            random.order = FALSE, 
            rot.per = 0.3, 
            colors = brewer.pal(8, "Dark2"))
  
 unigram_bar <- ggplot(word_freq %>% slice_max(n, n = 15), 
                        aes(x = reorder(word, n), y = n)) +
    geom_col(fill = "steelblue") +
    coord_flip() +
    labs(title = "Top Words in Headlines", x = "Words", y = "Frequency") +
    theme_minimal()
  
  wordcloud2::wordcloud2(bigrams %>% filter(n > 5), size = 0.5)
  
  plot_ngrams <- function(ngram_df, title_text) {
    ngram_df %>% 
      as_tibble() %>%  
      arrange(desc(n)) %>%  
      head(20) %>%  
      ggplot(aes(x = reorder(!!sym(names(ngram_df)[1]), n), y = n)) +  
      geom_col(fill = "steelblue") +
      coord_flip() +
      labs(title = title_text, x = "N-Gram", y = "Frequency") +
      theme_minimal()
  }
  
  trigram_bar <- plot_ngrams(trigrams, "Top 20 Trigrams")
  quadgram_bar <- plot_ngrams(quadgrams, "Top 20 Quadgrams")
  
  create_sankey <- function(ngram_df, n_parts) {
    splits <- paste0("word", 1:n_parts)
    ngram_split <- ngram_df %>%
      separate(col = !!sym(names(ngram_df)[1]), into = splits, sep = " ", remove = FALSE) %>%
      filter(!is.na(.[[2]]))
    
    links_list <- list()
    for(i in 1:(n_parts-1)) {
      links_list[[i]] <- ngram_split %>% 
        transmute(source = .[[splits[i]]],
                  target = .[[splits[i+1]]],
                  value = n)
    }
    links_df <- bind_rows(links_list)
    nodes_df <- data.frame(name = unique(c(links_df$source, links_df$target)))
    links_df <- links_df %>%
      mutate(source = match(source, nodes_df$name) - 1,
             target = match(target, nodes_df$name) - 1)
    
    sankeyNetwork(Links = links_df, Nodes = nodes_df, Source = "source", 
                  Target = "target", Value = "value", NodeID = "name", 
                  fontSize = 12, nodeWidth = 30)
  }
  
  trigram_sankey <- create_sankey(trigrams, 3)
  quadgram_sankey <- create_sankey(quadgrams, 4)
  
  pentagram_graph <- function(penta_df) {
    penta_split <- penta_df %>%
      separate(pentagram, into = c("w1", "w2", "w3", "w4", "w5"), sep = " ", remove = FALSE) %>%
      filter(!is.na(w2))
    g <- graph_from_data_frame(penta_split, directed = FALSE)
    ggraph(g, layout = "fr") +
      geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
      geom_node_point(color = "blue", size = 3) +
      geom_node_text(aes(label = name), repel = TRUE, size = 3) +
      theme_minimal()
  }
  
  pentagram_network <- pentagram_graph(pentagrams)
  
  hexagram_treemap <- ggplot(hexagrams, aes(area = n, fill = n, label = hexagram)) +
    geom_treemap() +
    geom_treemap_text(colour = "white", place = "centre") +
    scale_fill_viridis_c() +
    labs(title = "Hexagram Treemap") +
    theme_minimal()
  
  emotions <- tidy_df %>%
    inner_join(lexicon, by = "word") %>%
    group_by(id = row_number()) %>%
    summarize(across(Positive:Trust, sum, na.rm = TRUE))
  
  df_emotions <- df %>%
    mutate(id = row_number()) %>%
    left_join(emotions, by = "id")
  
  mean_emotions <- df_emotions %>%
    summarize(across(Positive:Trust, mean, na.rm = TRUE))
  
  emotion_bar <- ggplot(mean_emotions %>% pivot_longer(cols = everything(), names_to = "Emotion", values_to = "Mean"), 
                          aes(x = reorder(Emotion, -Mean), y = Mean, fill = Emotion)) +
    geom_bar(stat = "identity") +
    theme_minimal() +
    labs(title = "Mean Emotion Scores", x = "Emotion", y = "Mean Score") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  df_emotions_filtered <- df_emotions %>%
  filter(rowSums(select(., Positive:Trust) != 0) > 0)

emotion_heat_data <- as.matrix(df_emotions_filtered %>% select(Positive:Trust))

heatmap(emotion_heat_data, 
        scale = "column", 
        col = heat.colors(256), 
        margins = c(5, 10), 
        xlab = "Emotion", 
        ylab = "Headline",
        main = "Emotion Heatmap of Headlines")
  
  return(list(
    word_freq_table = word_freq,
    bigrams_table = bigrams,
    trigrams_table = trigrams,
    quadgrams_table = quadgrams,
    pentagrams_table = pentagrams,
    hexagrams_table = hexagrams,
    mean_emotion_table = mean_emotions,
    unigram_bar_plot = unigram_bar,
    trigram_bar_plot = trigram_bar,
    quadgram_bar_plot = quadgram_bar,
    trigram_sankey = trigram_sankey,
    quadgram_sankey = quadgram_sankey,
    pentagram_network = pentagram_network,
    hexagram_treemap = hexagram_treemap,
    emotion_bar_chart = emotion_bar
  ))
}


```

# RUNNING THE ANALYSIS FUNCTION ON 'THE BRIDGE' DATASET

```{r}

bridge_headlines_df <- read_csv("bridge_headlines.csv")
bridge_results <- process_headlines_full(bridge_headlines_df, headline, nrc_cleaned)

head(bridge_results$word_freq_table,60)
head(bridge_results$bigrams_table)
head(bridge_results$trigrams_table)
head(bridge_results$quadgrams_table)
head(bridge_results$pentagrams_table)
head(bridge_results$hexagrams_table)
bridge_results$mean_emotion_table

print(bridge_results$unigram_bar_plot)
print(bridge_results$trigram_bar_plot)
print(bridge_results$quadgram_bar_plot)
print(bridge_results$emotion_bar_chart)

```

# 'India', 'Asian' and 'World' are the most common words in this dataset, suggesting the importance of geographical entities in sports news. 'Team' (869 instances), Win (632 instances) and Gold (384 instances) are some common words which are not geographical entities or names of tournaments.

# The most common n-grams are names of tournaments which also act as SEO keywords - 'Olympics 2024', 'Asian Games', 'World Cup', 'FIH Pro League'. The one exception to this is 'Satwik Chirag', the name of a badminton doubles pair which appears 198 times in the 1200 headlines. The only names of people which appear more in this set of headlines are 'Sindhu' (234 times), 'Lakshya' (211 times) and 'Singh' (275 times), the last being a surname which could be associated with several people. The four badminton players Satwik, Chirag, Sindhu and Lakshya, therefore, can be said to be the equivalent of US Presidential candidates from the fake news dataset, though none of them have as much of a lead over the others as Trump did over his rivals.

# Emotion and sentiment scores: The positive sentiment (0.22) is present in this dataset far more than the negative sentiment (0.11). Trust (0.14) and anticipation (0.12) are the most common emotions. Disgust (0.02) and Sadness (0.05) are the least common emotions.

# Heatmap: Joy, anticipation and trust are the most commonly co-occurring emotions in particular headlines, and these headlines have a positive sentiment (Eg - Indian women's hockey squad for Junior Asia Cup announced). Sadness and anger are two co-occurring emotions on the other end of the heatmap, and these have a negative sentiment (Eg - Kush Maini completes another successful Formula 1 test, inches closer to the Indian F1 dream). In this dataset, the positive sentiment is the most independently occurring, which means that there are many headlines here which are positive but do not have scores for any of the 8 emotions (Eg - India edge-out China to defend Asian Champions Trophy).

# THREE-WORD PHRASES

```{r}

top_connections <- bridge_results$trigram_sankey$x$links %>%
  dplyr::arrange(desc(value)) %>%
  head(100)
bridge_results$trigram_sankey$x$links <- top_connections
links <- bridge_results$trigram_sankey$x$links
unique_nodes <- unique(c(links$source, links$target))
node_names <- bridge_results$trigram_sankey$x$nodes$name
links$source_word <- node_names[links$source + 1]  
links$target_word <- node_names[links$target + 1]  

plot <- plot_ly(
  type = "sankey",
  node = list(
    pad = 15,
    thickness = 20,
    line = list(color = "black", width = 0.5),
    label = node_names  # Use words as node labels
  ),
  link = list(
    source = match(links$source_word, node_names) - 1,  # Use the words' indices in node_names
    target = match(links$target_word, node_names) - 1,  # Use the words' indices in node_names
    value = links$value
  )
)

plot

```


# FOUR-WORD PHRASES

```{r}

top_quadgram_connections <- bridge_results$quadgram_sankey$x$links %>%
  dplyr::arrange(desc(value)) %>%
  head(100)
bridge_results$quadgram_sankey$x$links <- top_quadgram_connections
links_quadgram <- bridge_results$quadgram_sankey$x$links
node_names_quadgram <- bridge_results$quadgram_sankey$x$nodes$name
links_quadgram$source_word <- node_names_quadgram[links_quadgram$source + 1]  
links_quadgram$target_word <- node_names_quadgram[links_quadgram$target + 1]  

plot_quadgram <- plot_ly(
  type = "sankey",
  node = list(
    pad = 15,
    thickness = 20,
    line = list(color = "black", width = 0.5),
    label = node_names_quadgram  
  ),
  link = list(
    source = match(links_quadgram$source_word, node_names_quadgram) - 1,  
    target = match(links_quadgram$target_word, node_names_quadgram) - 1,  
    value = links_quadgram$value
  )
)

plot_quadgram

```

# The above Sankey diagrams show the mapping between words in common phrases. Most of the words mapped here are part of tournament names or federation names. The one exception is Rohan Bopanna-Matthew Ebden, a tennis doubles duo.

# This dataset contains 73,158 unique pentagrams and 81,970 unique hexagrams, but none of them have a frequency count of more than 1.

